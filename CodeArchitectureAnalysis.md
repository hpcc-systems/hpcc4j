# HPCC4J Code Architecture Analysis
**For AI Agents: Bug Fixing Reference Guide**

---

## 1. Project Overview

HPCC4J is a multi-module Maven project (Java 8+) providing libraries for interacting with HPCC Systems clusters. Version: 10.0.7-0-SNAPSHOT.

**Core Modules:**
- `commons-hpcc` - Shared utilities and data structures
- `wsclient` - Web service client layer  
- `dfsclient` - Direct file system access
- `spark-hpcc` - Apache Spark integration
- `clienttools` - ECL compiler interface
- `rdf2hpcc` - RDF data ingestion

---

## 2. Critical Architecture Patterns

### 2.1 Autogenerated Code Hierarchy (wsclient)

**NEVER MODIFY GENERATED CODE DIRECTLY**

```
org.hpccsystems.ws.client.gen.axis2.*
  └─ Raw Axis2 ADB stubs (AUTOGENERATED from WSDL)
     ├─ Generated by axis2-wsdl2code-maven-plugin
     ├─ 14 different web services (wsdfu, wsworkunits, wssmc, etc.)
     └─ 100+ classes per service

org.hpccsystems.ws.client.wrappers.gen.*
  └─ Wrapper classes (PROGRAMMATICALLY GENERATED)
     ├─ Shield callers from Axis2 interface changes
     ├─ Provide stable API surface
     └─ Example: ECLWorkunitWrapper wraps gen.axis2.wsworkunits.latest.ECLWorkunit

org.hpccsystems.ws.client.*
  └─ High-level client classes (HAND-WRITTEN)
     ├─ All extend BaseHPCCWsClient
     ├─ HPCCWsClient, HPCCWsDFUClient, HPCCWsWorkUnitsClient, etc.
     └─ 13 service-specific clients
```

**Gotcha:** When WSDL changes, both Axis2 stubs AND wrappers regenerate. Bug fixes must go in high-level clients or wrapper generation logic, never in generated code.

**Generation Tool:** `wsclient/utils/wsInterfaceUpdater.py` - Python script that:
1. Fetches WSDLs from ESP or generates from ESDL
2. Runs Axis2 codegen
3. Programmatically generates wrapper classes

### 2.2 Connection & Client Management

```java
// Pattern: Connection objects encapsulate endpoint + auth
Connection conn = new Connection("http://myhpcc:8010");
conn.setCredentials(username, password);

// Pattern: Object pooling for client reuse
HPCCWsClientPool pool = new HPCCWsClientPool(conn);
HPCCWsClient client = pool.checkOut();
try {
    // Use client
} finally {
    pool.checkIn(client);
}
```

**Key Classes:**
- `Connection` - Endpoint + credentials + URL parsing utilities
- `BaseHPCCWsClient` - Abstract base for all service clients
  - Configures Axis2 Options (auth, timeouts, endpoints)
  - Contains OpenTelemetry tracing setup
  - Manages HPCC version detection

**Gotcha:** All service clients expect a `Connection` object. Direct instantiation of Axis2 stubs is unsupported.

---

## 3. Data Schema Architecture (commons-hpcc)

### 3.1 FieldDef - Core Data Structure

`FieldDef` represents HPCC record schemas:

```java
public class FieldDef {
    private String fieldName;
    private FieldType fieldType;        // enum: INTEGER, STRING, RECORD, DATASET, SET, etc.
    private String typeName;            // For composite types
    private FieldDef[] defs;            // Child fields (for RECORD/DATASET/SET)
    private HpccSrcType srcType;        // Source encoding (LITTLE_ENDIAN, UTF8, etc.)
    private long len;                   // Field length
    private boolean fixedLength;
    private boolean isUnsigned;
    private boolean isBlob;
    // ...
}
```

**Gotcha:** `len` may be non-zero but `fixedLength=false` for variable-length fields. Always check both.

**Critical Enum:** `FieldType` - Defines all HPCC data types (INTEGER, REAL, DECIMAL, STRING, RECORD, DATASET, SET, UNKNOWN, etc.)

### 3.2 ECL Record Definition Parsing

**ANTLR4 Grammar:** `wsclient/src/main/java/org/hpccsystems/ws/client/antlr/EclRecord.g4`

**Autogenerated Parser Classes** (DO NOT MODIFY):
```
wsclient/src/main/java/org/hpccsystems/ws/client/antlr/gen/
  ├─ EclRecordParser.java
  ├─ EclRecordLexer.java
  ├─ EclRecordBaseListener.java
  └─ EclRecordListener.java
```

**Hand-written:** `EclRecordReader.java` - Uses generated parser to build FieldDef trees from ECL RECORD definitions.

---

## 4. Spark Integration Architecture (spark-hpcc)

### 4.1 Core RDD Implementation

```
HpccRDD extends RDD<Row>
  ├─ Reads HPCC datasets as Spark Rows
  ├─ Manages partitions via InternalPartition (wraps DataPartition)
  ├─ Uses BinaryRecordReader from dfsclient
  └─ Supports column projection and filtering
```

**Key Classes:**
- `HpccFile` - Entry point for reading
- `HpccRDD` - Custom RDD implementation
- `HpccFileWriter` - Writing Spark DataFrames to HPCC
- `SparkSchemaTranslator` - Converts FieldDef ↔ Spark StructType

**DataSource API Support:**
```java
// spark-hpcc/src/main/scala/org/hpccsystems/spark/datasource/
// Enables .format("hpcc") in Spark
```

### 4.2 Schema Translation Gotchas

**Unsigned8 Handling:** HPCC UNSIGNED8 → Spark LongType
- Can overflow to negative in Java long
- Use decimal conversion if precision matters

**String Processing Flags:**
- `BinaryRecordReader.NO_STRING_PROCESSING` - Default
- TRIM/PAD options available via `stringProcessingFlags`

---

## 5. File System Client Architecture (dfsclient)

### 5.1 Reading Pattern

```java
HpccRemoteFileReader<HPCCRecord> reader = new HpccRemoteFileReader(conn, filename, cluster, recordDef);
DataPartition[] parts = reader.getFileParts();

for (DataPartition part : parts) {
    BinaryRecordReader recordReader = new BinaryRecordReader(part);
    while (recordReader.hasNext()) {
        Object[] record = recordReader.next();
    }
}
```

**Key Interfaces:**
- `IRecordReader` - Sequential record access
- `IRecordAccessor` - Extract fields from records (generic or reflection-based)
- `IRecordBuilder` - Construct records for writing

### 5.2 Writing Pattern

```java
HPCCRemoteFileWriter writer = new HPCCRemoteFileWriter(conn, filename, cluster, recordDef);
BinaryRecordWriter recordWriter = writer.getBinaryRecordWriter(partIndex);
recordWriter.writeRecord(recordData);
recordWriter.close();
```

**File Formats Supported:**
- HPCC Binary (native)
- Parquet via Avro serialization
- Generic via IRecordBuilder/IRecordAccessor

---

## 6. Cross-Cutting Concerns

### 6.1 OpenTelemetry Tracing

**Global Instrumentation:**
```java
@WithSpan  // Automatically creates trace spans
public void someMethod() { ... }
```

**Used Extensively In:**
- All BaseHPCCWsClient methods
- Connection establishment
- File operations
- Pool management

**Gotcha:** When adding new public methods to ws clients, include `@WithSpan` for distributed tracing continuity.

### 6.2 Error Handling

**HPCC-Specific Exceptions:**
```java
ArrayOfEspExceptionWrapper - ESP service errors
ArrayOfECLExceptionWrapper - ECL execution errors  
EspSoapFaultWrapper - SOAP faults
```

**Pattern:** Methods throw `Exception` + typed wrapper exceptions
```java
public Result method() throws Exception, ArrayOfEspExceptionWrapper
```

### 6.3 Version Compatibility

**Breaking Change in v7.6.0:** Direct SoapStub access removed
- OLD (unsupported): `stub.someMethod()`
- NEW (required): Use wrapper classes or high-level clients

**Breaking Change in v10.0.0:** Dropped HPCC Platform v7.0.0 support
- Removed `org.hpccsystems.ws.client.gen.axis2.wsdfu.v1_39.*`
- See MIGRATION-10.0.md for migration paths

**Compatibility Matrix:**
- HPCC4J 10.x → HPCC Platform 7.0.2+ / 8.x / 9.x
- API versions sync with platform versions

---

## 7. Build & Test Infrastructure

### 7.1 Maven Profiles

**Default Build:** `mvn install`
- Runs base tests only (`@org.hpccsystems.commons.annotations.BaseTests`)
- Java 8 target (`maven.compiler.release=8`)

**Release Build:** `mvn clean deploy -P release`
- Requires GPG signing credentials
- Publishes to Maven Central via Sonatype

**Jenkins Profiles:**
- `jenkins-on-demand` - Runs BaseTests + RemoteTests
- `jenkins-release` - Release with BaseTests only
- `known-server-issues` - Tests for known HPCC bugs

**Benchmark Profile:** `mvn install -P benchmark`
- Runs `@org.hpccsystems.commons.annotations.Benchmark` tests

### 7.2 Test Annotations

```java
@org.hpccsystems.commons.annotations.BaseTests          // Default
@org.hpccsystems.commons.annotations.RemoteTests       // Requires HPCC cluster
@org.hpccsystems.commons.annotations.Benchmark         // Performance tests
@org.hpccsystems.commons.annotations.KnownServerIssueTests
```

**Gotcha:** ALL new ws.client tests should extend `BaseHPCCWsClientTest` (if it exists, verify path)

### 7.3 Code Formatting

**MANDATORY:** Eclipse formatter at `/eclipse/HPCC-JAVA-Formatter.xml`
- 4 spaces, no tabs
- Apache 2.0 license headers required
- New files must use current year in header

---

## 8. Common Bug Patterns & Gotchas

### 8.1 WSDL/Wrapper Mismatches

**Symptom:** NoSuchMethodError at runtime
**Cause:** Axis2 stubs regenerated but wrappers not updated
**Fix:** Run `wsInterfaceUpdater.py` to regenerate both layers

### 8.2 File Path Format Issues

**Legacy:** `namespace::filename` (e.g., `spark::test::dataset`)
**Modern:** `/namespace/filename` (preferred for Databricks)

**Gotcha:** Both formats supported but translation logic in `Connection.parsePath()` can fail on malformed inputs

### 8.3 Connection Timeout Issues

**Default:** 120 seconds for file operations
**Configurable:** Via `HpccRDD.setConnectionTimeout()` or connection options

**Symptom:** SocketTimeoutException on large file reads
**Fix:** Increase timeout for long-running operations

### 8.4 Unsigned Type Overflow

**HPCC UNSIGNED8** (0 to 18,446,744,073,709,551,615)
↓
**Java long** (-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807)

**Gotcha:** Values > Long.MAX_VALUE appear negative. Use BigDecimal conversion for accurate representation.

### 8.5 OpenTelemetry Context Loss

**Symptom:** Broken trace continuity
**Fix:** Ensure `@WithSpan` on all public API methods that may be traced

## 9. Module Dependencies

```
commons-hpcc
  ├─ No internal dependencies
  └─ Base types: FieldDef, FieldType, RecordDefinitionTranslator

wsclient
  ├─ Depends on: commons-hpcc
  ├─ Uses: Axis2 ADB, ANTLR4 (for ECL parsing)
  └─ Provides: Connection, BaseHPCCWsClient, service clients

dfsclient  
  ├─ Depends on: commons-hpcc, wsclient (for Connection)
  ├─ Uses: Parquet, Avro, Hadoop libraries
  └─ Provides: BinaryRecordReader/Writer, HPCCFile access

spark-hpcc
  ├─ Depends on: commons-hpcc, dfsclient
  ├─ Uses: Apache Spark 3.x
  └─ Provides: HpccRDD, HpccFileWriter, DataSource API

clienttools
  ├─ Depends on: commons-hpcc
  └─ Provides: eclcc Java interface

rdf2hpcc
  ├─ Depends on: wsclient
  ├─ Uses: Apache Jena
  └─ Provides: RDF ingestion tools
```

---

## 10. Key Files for Bug Investigation

### Configuration
- `pom.xml` (root) - Version properties, dependency management
- `wsclient/pom.xml` - Axis2 codegen configuration (14 executions)

### Autogenerated (DO NOT DEBUG DIRECTLY)
- `wsclient/src/main/java/org/hpccsystems/ws/client/gen/axis2/**/*.java`
- `wsclient/src/main/java/org/hpccsystems/ws/client/wrappers/gen/**/*.java`
- `wsclient/src/main/java/org/hpccsystems/ws/client/antlr/gen/*.java`

### Core Logic (DEBUG HERE)
- `BaseHPCCWsClient.java` - Client initialization, auth, version detection
- `Connection.java` - Endpoint parsing, credential encoding (1115 lines)
- `FieldDef.java` - Schema definition (571 lines)
- `HpccRDD.java` - Spark partition management (332 lines)
- `BinaryRecordReader.java` - Binary format parsing

### Utilities
- `wsclient/utils/wsInterfaceUpdater.py` - WSDL/stub regeneration
- `eclipse/HPCC-JAVA-Formatter.xml` - Code style enforcement

---

## 11. Debugging Checklist

**When fixing a bug, verify:**

1. **Is the affected code autogenerated?**
   - If yes → Fix generator or high-level wrapper, not generated code

2. **Does the fix involve web service changes?**
   - Check if WSDL version changed
   - Regenerate stubs and wrappers via `wsInterfaceUpdater.py`

3. **Does it affect data types/schemas?**
   - Review FieldDef construction
   - Check ECL RECORD parsing in EclRecordReader
   - Verify unsigned type handling

4. **Is it a connection/auth issue?**
   - Debug in Connection.java or BaseHPCCWsClient
   - Check Axis2 Options configuration
   - Verify endpoint URL parsing

5. **Does it involve Spark?**
   - Check schema translation (SparkSchemaTranslator)
   - Verify partition management in HpccRDD
   - Review BinaryRecordReader usage

6. **Did you add `@WithSpan` if it's a new public method?**
   - Required for trace continuity

7. **Did you run the formatter?**
   - Use Eclipse with HPCC-JAVA-Formatter.xml

8. **Did you add/update license headers?**
   - Apache 2.0 with current year

9. **Did you add appropriate test annotations?**
   - `@BaseTests` for unit tests
   - `@RemoteTests` if HPCC cluster needed

10. **Is the fix compatible with Java 8?**
    - No Java 9+ features

---

## 12. Related Documentation

- **Migration Guide:** MIGRATION-10.0.md (v7.0.0 removal details)
- **WSDL Generator:** wsclient/utils/README.md
- **Spark Examples:** spark-hpcc/Examples/PySparkExample.ipynb
- **Main README:** README.md (build instructions, module overview)
- **Issue Tracker:** https://track.hpccsystems.com/browse/JAPI

---

**Document Version:** 1.0  
**Last Updated:** 2025-11-11  
**Target Audience:** AI agents performing bug fixes on HPCC4J codebase
