# AI Prompt: Generate Comprehensive Test Case Analysis for HPCC Java Client

## üìã Quick Reference

**‚ö†Ô∏è CRITICAL: Analyze Existing Tests First**
- **Before generating any test cases**, review all existing test methods for the target method
- **Document what is already tested** in the "Existing Test Coverage Analysis" section
- **Only generate test cases for scenarios NOT covered** by existing tests
- If an existing test adequately covers a scenario, **DO NOT** create a duplicate test case

**Test Categories:**
1. **Core Functionality Tests (CFT)**: Basic operation, complete requests, data variations, typical workflows
2. **Edge Case Tests (ECT)**: Boundary values, optional parameters, unusual inputs, performance limits  
3. **Error Handling Tests (EHT)**: Invalid inputs, server-side errors, client-side errors

**Dataset Integration:**
- Review existing datasets from `generate-datasets.ecl` before defining tests
- Map each test to an existing dataset OR specify new dataset requirements
- Available datasets: `~benchmark::all_types::200KB`, `~benchmark::string::100MB`, `~benchmark::integer::20KB`, `~benchmark::all_types::superfile`, `~benchmark::integer::20kb::key`

**Output Structure:**
1. Method Summary
2. **Existing Test Coverage Analysis** ‚ö†Ô∏è NEW - Document what's already tested
3. Request Structure  
4. Server Behavior and Responses
5. Error Handling
6. Existing Dataset Analysis
7. Test Case Plan (organized by category with dataset mappings) - **ONLY for gaps in coverage**
8. New Dataset Specifications (if needed)

---

## üéØ Objective

You are an AI agent responsible for generating comprehensive test case analyses for a Java client library that exposes remote server APIs.

The goal is to:

- Understand the purpose and behavior of a given method
- Analyze related request/response structures and server-side implementations  
- Produce a detailed analysis file describing all test scenarios (valid, invalid, and error cases) that should be covered

## üß© System Context

### Client-Side (Java)

- The client library uses Apache Axis2 to generate request and response objects from WSDL definitions
- These objects are wrapped by autogenerated wrapper classes to avoid direct dependency on Axis2 classes
- Each server-side API group is represented by a client class of the form: `HPCC${ServiceName}Client`

### Server-Side (C++)

- Service implementations are defined in: `esp/services/ws_${ServiceName}`
- Request and response object definitions (IDL) are in: `esp/scm/ws_${ServiceName}.ecm`

## üß™ Testing Guidelines

### Test Location
Tests must be placed under: `${ServiceName}ClientTest.java`

If this file does not exist, it should be created.

### Test Type Preferences
- Both unit and integration tests may be generated, but **integration tests are preferred** since test environments are easily deployable
- **Mocks must not be used**, as live test environments are available

### Test Coverage Requirements

Test cases should fully cover:

- Valid inputs and normal operation
- Invalid inputs and expected error responses  
- Exceptional conditions or failure scenarios

## üß† Task Instructions

You will be provided a method that lacks test coverage or has insufficient coverage.
Follow the steps below carefully.

### Step 1 ‚Äî Analyze the Codebase

1. Review the `HPCC${ServiceName}Client` Java class
2. Identify the target `${MethodName}` method and examine its logic, parameters, and return type
3. Inspect the corresponding Java request and response wrapper classes for this method
4. Review the server-side C++ implementation located in `esp/services/ws_${ServiceName}`
5. Reference the IDL definition (`esp/scm/ws_${ServiceName}.ecm`) to understand required/optional fields and constraints
6. **Review existing test files** (`${ServiceName}ClientTest.java`) to:
   - Understand current test patterns and dataset usage
   - **Identify all existing test methods for `${MethodName}`**
   - **Analyze what scenarios are already covered by existing tests**
   - **Document which test cases DO NOT need to be generated because they already exist**

### Step 2 ‚Äî Create the Analysis Output

Generate a file named: `${ServiceName}.${MethodName}Analysis.md`

The output file must include the following Markdown-formatted sections:

#### 1. Method Summary
- Purpose of the method
- Overview of its role within the service
- Description of inputs, outputs, and side effects

#### 2. Existing Test Coverage Analysis

**CRITICAL**: Before defining any new test cases, analyze all existing tests for this method.

For each existing test found:

| Existing Test Method Name | Test Category | Scenario Covered | Input Data Summary | Pass Criteria | Notes |
|---------------------------|---------------|------------------|---------------------|---------------|-------|
| ... | CFT/ECT/EHT | ... | ... | ... | ... |

**Coverage Summary:**
- Total existing test methods: [number]
- Core Functionality Tests covered: [count and brief description]
- Edge Case Tests covered: [count and brief description]
- Error Handling Tests covered: [count and brief description]
- **Gaps identified**: [List scenarios that are NOT covered by existing tests]

**Important**: Any test scenario that is adequately covered by an existing test **MUST NOT** be included in the new test case plan (Step 7). Only generate test cases for scenarios that have gaps or no coverage.

#### 3. Request Structure
- List all request fields in a table:

| Field Name | Type | Required | Description | Valid Range / Format | Notes |
|------------|------|----------|-------------|---------------------|-------|
| ... | ... | ... | ... | ... | ... |

- Identify dependencies between fields (e.g., conditional requirements)
- Specify default behavior for optional fields

#### 4. Server Behavior and Responses
- Describe the server-side logic for processing the request
- List all valid response types and their meanings
- List possible invalid responses, errors, or exceptions

#### 5. Error Handling
- **Server-side errors**: validation issues, missing data, or internal failures
- **Client-side errors**: API misuse, unexpected responses, or communication failures

#### 6. Existing Dataset Analysis

Review available test datasets and their applicability:

| Dataset Name | Applicable? | Reason |
|-------------|-------------|--------|
| `~benchmark::all_types::200KB` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::string::100MB` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::integer::20KB` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::all_types::superfile` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::integer::20kb::key` | Yes/No | [Explain why it can or cannot be used] |

#### 7. Test Case Plan

**CRITICAL REQUIREMENT**: Only define test cases for scenarios that are **NOT** already covered by existing tests (as documented in Step 2 - Existing Test Coverage Analysis).

**Before including any test case below:**
1. Verify it was not listed in the "Existing Test Coverage Analysis" section
2. Verify it addresses a gap identified in the coverage summary
3. If a similar test exists, explain why the new test is still necessary (e.g., different dataset, different edge case variant)

Test cases must be organized into three primary categories with clearly labeled subcategories:

**Test ID Convention:**
- Core Functionality Tests: `CFT-001`, `CFT-002`, etc.
- Edge Case Tests: `ECT-001`, `ECT-002`, etc.
- Error Handling Tests: `EHT-001`, `EHT-002`, etc.

##### A. Core Functionality Tests
Tests that verify the method's primary purpose and expected normal operation:

- **Basic Operation**: Standard use case with minimal required parameters
- **Complete Request**: Use case including all optional fields
- **Data Variations**: Different valid data types, formats, or values
- **Typical Workflows**: Common patterns of method usage

##### B. Edge Case Tests
Tests that explore boundary conditions and unusual but valid scenarios:

- **Boundary Values**: Minimum, maximum, empty, or very large inputs
- **Optional Parameters**: Various combinations of optional fields
- **Unusual Valid Inputs**: Valid but uncommon data patterns
- **Performance Limits**: Large datasets, many records, or extended operations

##### C. Error Handling Tests
Tests that verify proper handling of invalid inputs and error conditions:

- **Invalid Inputs**: 
  - Missing required fields
  - Malformed or incorrectly typed values
  - Out-of-range values
  - Conflicting parameter combinations
  
- **Server-Side Errors**:
  - Validation failures
  - Resource not found
  - Permission/authorization issues
  - Internal server errors
  
- **Client-Side Errors**:
  - Null/invalid connection
  - Timeout scenarios
  - Unexpected response formats
  - Communication failures

**Test Case Specification Format:**

For each test case, provide:

| Field | Description |
|-------|-------------|
| **Test ID** | Unique identifier (e.g., `CFT-001` for Core Functionality Test) |
| **Category** | Core Functionality / Edge Case / Error Handling |
| **Subcategory** | Specific classification from above |
| **Description** | Brief summary of what is being tested |
| **Input Data** | Complete request fields and values |
| **Dataset** | Either: existing dataset name (e.g., `~benchmark::integer::20KB`) OR `[NEW DATASET REQUIRED]` with specifications |
| **Expected Output** | Response values, exception type, or error code |
| **Pass Criteria** | Specific conditions that indicate success |
| **Notes** | Additional context, dependencies, or setup requirements |

**Dataset Requirements Format (for new datasets):**

When a new dataset is required, include:

```
Dataset Name: ~test::${purpose}::${type}
Required For: [Test IDs that need this dataset]
Structure:
  - Field definitions and types
  - Record count
  - Special characteristics (nulls, duplicates, specific patterns)
Rationale: Why existing datasets cannot be used
```

**Example Test Case:**

| Field | Description |
|-------|-------------|
| **Test ID** | CFT-001 |
| **Category** | Core Functionality |
| **Subcategory** | Basic Operation |
| **Description** | Retrieve data columns from a simple integer dataset |
| **Input Data** | `fileName`: `~benchmark::integer::20KB`, `cluster`: `mythor` |
| **Dataset** | `~benchmark::integer::20KB` (existing) |
| **Expected Output** | Response with 2 columns: `key` (integer) and `fill` (integer), recordCount: 1250 |
| **Pass Criteria** | - Response is not null<br>- Column count equals 2<br>- Column names and types match expectations<br>- No exceptions thrown |
| **Notes** | This is the simplest valid case with minimal parameters |

**Example New Dataset Requirement:**

```
Dataset Name: ~test::null_values::mixed
Required For: EHT-015, EHT-016, EHT-017
Structure:
  - Field1: STRING (50% null values)
  - Field2: INTEGER (25% null values)  
  - Field3: REAL (no null values)
  - Record count: 1000
  - Special characteristics: Must include both null and non-null values to test null handling
Rationale: Existing datasets don't contain null values, which are critical for testing null-handling logic in the method
```

#### 8. New Dataset Specifications

If any test cases require new datasets, consolidate all requirements here with complete ECL code snippets for dataset generation.

### Step 3 ‚Äî Review Available Test Datasets

Before defining test cases, review the available test datasets from `generate-datasets.ecl`:

**Available Datasets:**

| Dataset Name | Description | Key Characteristics |
|-------------|-------------|---------------------|
| `~benchmark::all_types::200KB` | Multi-column dataset with various data types | Includes: int8, uint8, int4, uint4, int2, uint2, r8, r4, dec16, udec16, qStr, fixStr8, str, varStr, varStr8, utfStr, uni8, uni, varUni, childDataset, int1Set (nested records and sets); ~5600 records |
| `~benchmark::string::100MB` | Large string-based dataset | Two string8 columns: key and fill; 6,250,000 records |
| `~benchmark::integer::20KB` | Small integer dataset | Two integer columns: key and fill; 1,250 records |
| `~benchmark::all_types::superfile` | Superfile containing subfiles | Contains `~benchmark::all_types::200KB` as subfile |
| `~benchmark::integer::20kb::key` | Index file | Index on integer dataset |

**Dataset Location:**
- ECL generation script: `wsclient/src/test/resources/generate-datasets.ecl` or `dfsclient/src/test/resources/generate-datasets.ecl`

**Dataset Analysis Requirements:**

For each test case you define in Step 2, you must identify:
1. Whether an existing dataset can be used
2. If a new dataset is needed, specify:
   - Required data types and structure
   - Size/record count requirements
   - Special characteristics (e.g., null values, edge cases, specific patterns)
   - Suggested dataset name following convention: `~test::${purpose}::${type}`

### Step 4 ‚Äî Output Requirements

- Output must be Markdown formatted
- Use clear headings, tables, and lists for readability
- Be exhaustive ‚Äî the analysis should allow a developer to write complete test cases without further clarification
- Do not omit any potential error or edge case observed during analysis

## ‚úÖ Goal

By the end of this task, you will have produced a detailed analysis file: `${ServiceName}.${MethodName}Analysis.md`

This file will:

- Document the functional behavior of the method
- **Catalog all existing test coverage for this method**
- **Identify gaps in existing test coverage**
- Identify all request/response constraints
- Enumerate valid and invalid test scenarios categorized by test type **that are not already covered by existing tests**
- Map test cases to existing datasets or define new dataset requirements
- Provide a foundation for generating integration tests with full coverage **without duplicating existing tests**
