# AI Prompt: Generate Comprehensive Test Case Analysis for HPCC Java Client

## ðŸ“‹ Quick Reference

**Test Categories:**
1. **Core Functionality Tests (CFT)**: Basic operation, complete requests, data variations, typical workflows
2. **Edge Case Tests (ECT)**: Boundary values, optional parameters, unusual inputs, performance limits  
3. **Error Handling Tests (EHT)**: Invalid inputs, server-side errors, client-side errors

**Dataset Integration:**
- Review existing datasets from `generate-datasets.ecl` before defining tests
- Map each test to an existing dataset OR specify new dataset requirements
- Available datasets: `~benchmark::all_types::200KB`, `~benchmark::string::100MB`, `~benchmark::integer::20KB`, `~benchmark::all_types::superfile`, `~benchmark::integer::20kb::key`

**Output Structure:**
1. Method Summary
2. Request Structure  
3. Server Behavior and Responses
4. Error Handling
5. Existing Dataset Analysis
6. Test Case Plan (organized by category with dataset mappings)
7. New Dataset Specifications (if needed)

---

## ðŸŽ¯ Objective

You are an AI agent responsible for generating comprehensive test case analyses for a Java client library that exposes remote server APIs.

The goal is to:

- Understand the purpose and behavior of a given method
- Analyze related request/response structures and server-side implementations  
- Produce a detailed analysis file describing all test scenarios (valid, invalid, and error cases) that should be covered

## ðŸ§© System Context

### Client-Side (Java)

- The client library uses Apache Axis2 to generate request and response objects from WSDL definitions
- These objects are wrapped by autogenerated wrapper classes to avoid direct dependency on Axis2 classes
- Each server-side API group is represented by a client class of the form: `HPCC${ServiceName}Client`

### Server-Side (C++)

- Service implementations are defined in: `esp/services/ws_${ServiceName}`
- Request and response object definitions (IDL) are in: `esp/scm/ws_${ServiceName}.ecm`

## ðŸ§ª Testing Guidelines

### Test Location
Tests must be placed under: `${ServiceName}ClientTest.java`

If this file does not exist, it should be created.

### Test Type Preferences
- Both unit and integration tests may be generated, but **integration tests are preferred** since test environments are easily deployable
- **Mocks must not be used**, as live test environments are available

### Test Coverage Requirements

Test cases should fully cover:

- Valid inputs and normal operation
- Invalid inputs and expected error responses  
- Exceptional conditions or failure scenarios

## ðŸ§  Task Instructions

You will be provided a method that lacks test coverage or has insufficient coverage.
Follow the steps below carefully.

### Step 1 â€” Analyze the Codebase

1. Review the `HPCC${ServiceName}Client` Java class
2. Identify the target `${MethodName}` method and examine its logic, parameters, and return type
3. Inspect the corresponding Java request and response wrapper classes for this method
4. Review the server-side C++ implementation located in `esp/services/ws_${ServiceName}`
5. Reference the IDL definition (`esp/scm/ws_${ServiceName}.ecm`) to understand required/optional fields and constraints
6. Review existing test files (`${ServiceName}ClientTest.java`) to understand current test patterns and dataset usage

### Step 2 â€” Create the Analysis Output

Generate a file named: `${ServiceName}.${MethodName}Analysis.md`

The output file must include the following Markdown-formatted sections:

#### 1. Method Summary
- Purpose of the method
- Overview of its role within the service
- Description of inputs, outputs, and side effects

#### 2. Request Structure
- List all request fields in a table:

| Field Name | Type | Required | Description | Valid Range / Format | Notes |
|------------|------|----------|-------------|---------------------|-------|
| ... | ... | ... | ... | ... | ... |

- Identify dependencies between fields (e.g., conditional requirements)
- Specify default behavior for optional fields

#### 3. Server Behavior and Responses
- Describe the server-side logic for processing the request
- List all valid response types and their meanings
- List possible invalid responses, errors, or exceptions

#### 4. Error Handling
- **Server-side errors**: validation issues, missing data, or internal failures
- **Client-side errors**: API misuse, unexpected responses, or communication failures

#### 5. Existing Dataset Analysis

Review available test datasets and their applicability:

| Dataset Name | Applicable? | Reason |
|-------------|-------------|--------|
| `~benchmark::all_types::200KB` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::string::100MB` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::integer::20KB` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::all_types::superfile` | Yes/No | [Explain why it can or cannot be used] |
| `~benchmark::integer::20kb::key` | Yes/No | [Explain why it can or cannot be used] |

#### 6. Test Case Plan

Test cases must be organized into three primary categories with clearly labeled subcategories:

**Test ID Convention:**
- Core Functionality Tests: `CFT-001`, `CFT-002`, etc.
- Edge Case Tests: `ECT-001`, `ECT-002`, etc.
- Error Handling Tests: `EHT-001`, `EHT-002`, etc.

##### A. Core Functionality Tests
Tests that verify the method's primary purpose and expected normal operation:

- **Basic Operation**: Standard use case with minimal required parameters
- **Complete Request**: Use case including all optional fields
- **Data Variations**: Different valid data types, formats, or values
- **Typical Workflows**: Common patterns of method usage

##### B. Edge Case Tests
Tests that explore boundary conditions and unusual but valid scenarios:

- **Boundary Values**: Minimum, maximum, empty, or very large inputs
- **Optional Parameters**: Various combinations of optional fields
- **Unusual Valid Inputs**: Valid but uncommon data patterns
- **Performance Limits**: Large datasets, many records, or extended operations

##### C. Error Handling Tests
Tests that verify proper handling of invalid inputs and error conditions:

- **Invalid Inputs**: 
  - Missing required fields
  - Malformed or incorrectly typed values
  - Out-of-range values
  - Conflicting parameter combinations
  
- **Server-Side Errors**:
  - Validation failures
  - Resource not found
  - Permission/authorization issues
  - Internal server errors
  
- **Client-Side Errors**:
  - Null/invalid connection
  - Timeout scenarios
  - Unexpected response formats
  - Communication failures

**Test Case Specification Format:**

For each test case, provide:

| Field | Description |
|-------|-------------|
| **Test ID** | Unique identifier (e.g., `CFT-001` for Core Functionality Test) |
| **Category** | Core Functionality / Edge Case / Error Handling |
| **Subcategory** | Specific classification from above |
| **Description** | Brief summary of what is being tested |
| **Input Data** | Complete request fields and values |
| **Dataset** | Either: existing dataset name (e.g., `~benchmark::integer::20KB`) OR `[NEW DATASET REQUIRED]` with specifications |
| **Expected Output** | Response values, exception type, or error code |
| **Pass Criteria** | Specific conditions that indicate success |
| **Notes** | Additional context, dependencies, or setup requirements |

**Dataset Requirements Format (for new datasets):**

When a new dataset is required, include:

```
Dataset Name: ~test::${purpose}::${type}
Required For: [Test IDs that need this dataset]
Structure:
  - Field definitions and types
  - Record count
  - Special characteristics (nulls, duplicates, specific patterns)
Rationale: Why existing datasets cannot be used
```

**Example Test Case:**

| Field | Description |
|-------|-------------|
| **Test ID** | CFT-001 |
| **Category** | Core Functionality |
| **Subcategory** | Basic Operation |
| **Description** | Retrieve data columns from a simple integer dataset |
| **Input Data** | `fileName`: `~benchmark::integer::20KB`, `cluster`: `mythor` |
| **Dataset** | `~benchmark::integer::20KB` (existing) |
| **Expected Output** | Response with 2 columns: `key` (integer) and `fill` (integer), recordCount: 1250 |
| **Pass Criteria** | - Response is not null<br>- Column count equals 2<br>- Column names and types match expectations<br>- No exceptions thrown |
| **Notes** | This is the simplest valid case with minimal parameters |

**Example New Dataset Requirement:**

```
Dataset Name: ~test::null_values::mixed
Required For: EHT-015, EHT-016, EHT-017
Structure:
  - Field1: STRING (50% null values)
  - Field2: INTEGER (25% null values)  
  - Field3: REAL (no null values)
  - Record count: 1000
  - Special characteristics: Must include both null and non-null values to test null handling
Rationale: Existing datasets don't contain null values, which are critical for testing null-handling logic in the method
```

#### 7. New Dataset Specifications

If any test cases require new datasets, consolidate all requirements here with complete ECL code snippets for dataset generation.

### Step 3 â€” Review Available Test Datasets

Before defining test cases, review the available test datasets from `generate-datasets.ecl`:

**Available Datasets:**

| Dataset Name | Description | Key Characteristics |
|-------------|-------------|---------------------|
| `~benchmark::all_types::200KB` | Multi-column dataset with various data types | Includes: int8, uint8, int4, uint4, int2, uint2, r8, r4, dec16, udec16, qStr, fixStr8, str, varStr, varStr8, utfStr, uni8, uni, varUni, childDataset, int1Set (nested records and sets); ~5600 records |
| `~benchmark::string::100MB` | Large string-based dataset | Two string8 columns: key and fill; 6,250,000 records |
| `~benchmark::integer::20KB` | Small integer dataset | Two integer columns: key and fill; 1,250 records |
| `~benchmark::all_types::superfile` | Superfile containing subfiles | Contains `~benchmark::all_types::200KB` as subfile |
| `~benchmark::integer::20kb::key` | Index file | Index on integer dataset |

**Dataset Location:**
- ECL generation script: `wsclient/src/test/resources/generate-datasets.ecl` or `dfsclient/src/test/resources/generate-datasets.ecl`

**Dataset Analysis Requirements:**

For each test case you define in Step 2, you must identify:
1. Whether an existing dataset can be used
2. If a new dataset is needed, specify:
   - Required data types and structure
   - Size/record count requirements
   - Special characteristics (e.g., null values, edge cases, specific patterns)
   - Suggested dataset name following convention: `~test::${purpose}::${type}`

### Step 4 â€” Output Requirements

- Output must be Markdown formatted
- Use clear headings, tables, and lists for readability
- Be exhaustive â€” the analysis should allow a developer to write complete test cases without further clarification
- Do not omit any potential error or edge case observed during analysis

## âœ… Goal

By the end of this task, you will have produced a detailed analysis file: `${ServiceName}.${MethodName}Analysis.md`

This file will:

- Document the functional behavior of the method
- Identify all request/response constraints
- Enumerate valid and invalid test scenarios categorized by test type
- Map test cases to existing datasets or define new dataset requirements
- Provide a foundation for generating integration tests with full coverage
